# 🎮 TinyCPP-DDQN: 從零開始的 C++ 強化學習實作
> **Zero-Dependency Double DQN in C++** > 一個專為 C++ 使用者設計，不依賴任何外部 AI 框架（如 PyTorch/TensorFlow），純手作的深度強化學習教學專案。


## 📖 這是什麼？ (Introduction)
這個專案旨在用最純粹的 **C++ (STL)** 語言，展示 **Double Deep Q-Network (DDQN)** 演算法是如何運作的。

透過讓 AI 在一個終端機迷宮中學習如何避開陷阱並走到終點，我們希望能讓剛接觸強化學習 (RL) 的學生理解以下核心概念，而不需要被複雜的 Python 函式庫困惑，可以從頭理解一次。

### 為什麼用 C++？
* **校園學習**：因為學生時期主要學習的語言是C++，要快速理解 Python 、 TensorFlow 或 Keras 會有點困難，因此以此基礎為出發點。
* **去黑盒子化 (De-blackbox)**：我們沒有 `import keras`，所有的神經網路層、梯度更新、記憶體緩衝區都是手寫的。這能強迫你理解演算法的每一行邏輯。
* **效能與架構**：展示如何使用 C++ 的 Class 架構來模組化 AI 代理人 (Agent) 與環境 (Environment)。

---
## Q(t) = R + γ*Q(t)(s',argmaxQ(p)(s',a))
## 🧠 DDQN 原理：用白話文解釋 (How it works)

如果你不懂 DDQN，想像你在練習射箭，而你有**兩個教練**：

1.  **行動教練 第一個網路'選擇' (Policy Network)**：
    * 很年輕，反應快，負責告訴你「下一箭射哪裡比較好」。他每一秒都在學習新的狀態。
    * **對應程式碼**：`Agent.h` 中的 `policyNet`。
2.  **評分教練 第二個網路'評估' (Target Network)**：
    * 很資深，負責評估「剛剛那一箭射得好不好」。為了不讓你混亂，他的評分標準不會一直變，而是每隔一段時間（例如 10 分鐘或 10 次）才更新一次標準。
    * **對應程式碼**：`Agent.h` 中的 `targetNet`。
3.  **筆記本 '更新'(Replay Buffer)**：
    * 你把每一次練習的結果（風向、動作、得分）都寫在筆記本裡。晚上睡覺前，你會隨機翻開筆記本複習，而不是只記得最後一次練習。這能避免你「學了新的，忘了舊的」。
    * **對應程式碼**：`ReplayBuffer.h`。

---

## 📂 專案結構 (File Structure)

我們將 AI 的神經拆解成數個 C++ 檔案，讓邏輯更清晰：

| 檔案名稱 | 角色 | 功能解說 |

| **`DDQNmain.cpp`** | **主程式** | 負責控制訓練迴圈，紀錄 CSV 數據以便於驗證是否成效，並在螢幕上印出迷宮動畫。 |

| **`Environment.h`** | **世界** | 定義迷宮地圖、陷阱 `X`、終點 `G` 以及移動規則。 |

| **`Agent.h`** | **代理人** | 這是 AI 的大腦。包含 $\epsilon$-Greedy 策略（決定何時探索、何時利用）。 |

| **`Simplenet.h`** | **神經網路** | 模擬神經網路的運作。我們用 Q-Table 結構搭配梯度下降更新公式來實作。 |

| **`ReplayBuffer.h`** | **記憶體** | 使用 `std::deque` 實作先進先出 (FIFO) 的記憶池，並提供 `sample()` 隨機抽樣功能。 |

| **`plot.py`** | **繪製** | 用 Python 的 matplotlib 描繪整個訓練的成果到images，清楚了解是否有成功加強學習能力。 |


---

## 🛠️ 如何執行 (How to Run)

本專案使用 `Makefile` 進行管理，確保在 Windows (MinGW/w64devkit) 或 Linux/Mac 上都能一鍵執行。

### 1. 編譯專案
進入src資料夾中在終端機輸入：
make
